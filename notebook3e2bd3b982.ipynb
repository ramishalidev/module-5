{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2645886,"sourceType":"datasetVersion","datasetId":1608934,"isSourceIdPinned":false}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 0 â€” GPU check, installs, reproducibility\n# Make sure GPU is enabled in Kaggle: Settings -> Accelerator -> GPU\n\n# Basic checks\n!nvidia-smi\n\n# Install kagglehub if not present (used to download the Kaggle dataset)\n!pip install -q kagglehub\n\n# TensorFlow (Kaggle typically has TF preinstalled). If you want a specific version, uncomment:\n# !pip install -q \"tensorflow==2.14.0\"\n\n# Reproducibility\nimport random, os, numpy as np, tensorflow as tf\nSEED = 42\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\nprint(\"TensorFlow version:\", tf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T16:14:48.855211Z","iopub.execute_input":"2025-11-29T16:14:48.855564Z","iopub.status.idle":"2025-11-29T16:14:53.458437Z","shell.execute_reply.started":"2025-11-29T16:14:48.855537Z","shell.execute_reply":"2025-11-29T16:14:53.457400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2 â€” Create project folders (Kaggle adapted)\nimport os\n\n# Kaggle working directory\nPROJECT_DIR = \"/kaggle/working\"\n\n# Check for input dataset (standard Kaggle location)\nKAGGLE_INPUT_DIR = \"/kaggle/input/brain-tumor-mri-dataset\"\n\n# We will determine RAW_DIR dynamically. \n# If the dataset is added as input, use it. Otherwise, we'll download it to a temp location or working dir.\nif os.path.exists(KAGGLE_INPUT_DIR):\n    RAW_DIR = KAGGLE_INPUT_DIR\n    print(f\"Using Kaggle input dataset at: {RAW_DIR}\")\nelse:\n    # Fallback: will be handled in next cell via kagglehub\n    RAW_DIR = os.path.join(PROJECT_DIR, \"data\", \"raw\")\n    print(f\"Dataset not found in input. Will attempt download in next cell.\")\n\nPROCESSED_DIR = os.path.join(PROJECT_DIR, \"data\", \"processed\")\nMODEL_DIR = os.path.join(PROJECT_DIR, \"models\")\nREPORT_DIR = os.path.join(PROJECT_DIR, \"reports\")\nARTIFACTS_DIR = os.path.join(PROJECT_DIR, \"artifacts\")\n\nfor d in [PROCESSED_DIR, MODEL_DIR, REPORT_DIR, ARTIFACTS_DIR]:\n    os.makedirs(d, exist_ok=True)\n\n# If RAW_DIR is inside working (i.e. we need to download it), create it\nif RAW_DIR.startswith(PROJECT_DIR):\n     os.makedirs(RAW_DIR, exist_ok=True)\n\nprint(\"Project directories set up.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T16:14:53.464932Z","iopub.execute_input":"2025-11-29T16:14:53.465231Z","iopub.status.idle":"2025-11-29T16:14:53.495402Z","shell.execute_reply.started":"2025-11-29T16:14:53.465205Z","shell.execute_reply":"2025-11-29T16:14:53.494406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3 â€” Dataset setup\nimport shutil, os\nimport kagglehub\n\n# Check if we need to download\n# We check if RAW_DIR contains the expected 'Training' folder or if it's the input dir\nif not os.path.exists(os.path.join(RAW_DIR, \"Training\")):\n    print(\"Dataset not found in RAW_DIR. Downloading using kagglehub...\")\n    try:\n        path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n        print(\"Downloaded to:\", path)\n        RAW_DIR = path # Update RAW_DIR to point to the downloaded cache\n        print(f\"Updated RAW_DIR to: {RAW_DIR}\")\n    except Exception as e:\n        print(f\"Error downloading dataset: {e}\")\nelse:\n    print(f\"Dataset already present at {RAW_DIR}\")\n\n# No need to zip and copy to Drive on Kaggle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T16:14:53.496477Z","iopub.execute_input":"2025-11-29T16:14:53.496756Z","iopub.status.idle":"2025-11-29T16:14:53.504854Z","shell.execute_reply.started":"2025-11-29T16:14:53.496735Z","shell.execute_reply":"2025-11-29T16:14:53.503845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4 â€” show folder tree & counts\nfrom pathlib import Path\n\ndef walk(path):\n    for root, dirs, files in os.walk(path):\n        print(f\"DIR: {root} | FILES: {len(files)}\")\nwalk(RAW_DIR)\n\n# List training/testing subfolders if present\nfrom pprint import pprint\ntrain_dir = os.path.join(RAW_DIR, \"Training\")\ntest_dir = os.path.join(RAW_DIR, \"Testing\")\n\nif os.path.exists(train_dir):\n    classes = sorted([p.name for p in Path(train_dir).iterdir() if p.is_dir()])\n    print(\"Detected classes (Training):\", classes)\n    counts = {c: len(list(Path(train_dir).joinpath(c).glob(\"*\"))) for c in classes}\n    print(\"Training counts:\", counts)\nelse:\n    print(\"Training directory not found at:\", train_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T16:14:53.507121Z","iopub.execute_input":"2025-11-29T16:14:53.507389Z","iopub.status.idle":"2025-11-29T16:14:55.871842Z","shell.execute_reply.started":"2025-11-29T16:14:53.507369Z","shell.execute_reply":"2025-11-29T16:14:55.870700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5 â€” Create tf.data datasets (train and val/test)\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\nAUTOTUNE = tf.data.AUTOTUNE\nSEED = 42\n\n# Paths (assumes dataset has 'Training' and 'Testing' directories)\nTRAIN_DIR = train_dir\nVAL_DIR = test_dir\n\n# If dataset is not split, you can use validation_split in image_dataset_from_directory\ntrain_ds = image_dataset_from_directory(\n    TRAIN_DIR,\n    labels='inferred',\n    label_mode='int',\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=SEED\n)\n\nval_ds = image_dataset_from_directory(\n    VAL_DIR,\n    labels='inferred',\n    label_mode='int',\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(\"Classes:\", class_names, \"Num classes:\", num_classes)\n\n# Data augmentation + preprocessing\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.08),\n    layers.RandomZoom(0.08),\n    layers.RandomTranslation(0.04, 0.04)\n], name=\"data_augmentation\")\n\npreprocess_input = tf.keras.applications.efficientnet.preprocess_input\n\ndef prepare(ds, training=False):\n    ds = ds.map(lambda x, y: (tf.image.resize(x, IMG_SIZE), y), num_parallel_calls=AUTOTUNE)\n    if training:\n        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n    ds = ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n    return ds.cache().prefetch(AUTOTUNE)\n\ntrain_ds = prepare(train_ds, training=True)\nval_ds = prepare(val_ds, training=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T16:14:55.872795Z","iopub.execute_input":"2025-11-29T16:14:55.873016Z","iopub.status.idle":"2025-11-29T16:14:56.674628Z","shell.execute_reply.started":"2025-11-29T16:14:55.872998Z","shell.execute_reply":"2025-11-29T16:14:56.673742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6 â€” Compute class weights using training folder counts\nfrom collections import Counter\nimport numpy as np\ntrain_counts = {c: len(list(Path(TRAIN_DIR).joinpath(c).glob(\"*\"))) for c in class_names}\nprint(\"Train counts:\", train_counts)\ntotal = sum(train_counts.values())\nclass_weight = {i: total/(num_classes * train_counts[class_names[i]]) for i in range(num_classes)}\nprint(\"Class weights:\", class_weight)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T16:14:56.675448Z","iopub.execute_input":"2025-11-29T16:14:56.675734Z","iopub.status.idle":"2025-11-29T16:14:56.700383Z","shell.execute_reply.started":"2025-11-29T16:14:56.675714Z","shell.execute_reply":"2025-11-29T16:14:56.699378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7 â€” Build hybrid model with PRESERVED SPATIAL GRADIENTS for Grad-CAM\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\ndef transformer_encoder(inputs, head_size=64, num_heads=4, ff_dim=128, dropout=0.1):\n    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(x, x)\n    x = layers.Dropout(dropout)(x)\n    x = layers.Add()([x, inputs])\n    y = layers.LayerNormalization(epsilon=1e-6)(x)\n    y = layers.Dense(ff_dim, activation=\"relu\")(y)\n    y = layers.Dropout(dropout)(y)\n    y = layers.Dense(inputs.shape[-1])(y)\n    out = layers.Add()([x, y])\n    return out\n\ndef build_hybrid_model_gradcam(input_shape=(224,224,3), num_classes=4, dropout_rate=0.3):\n    \"\"\"\n    Modified architecture that preserves spatial gradient flow for Grad-CAM.\n    \n    Key changes from original:\n    - Added 1x1 conv layers BEFORE GlobalAveragePooling\n    - This creates a gradient path: output -> transformer -> GAP -> conv -> spatial features\n    - The 'gradcam_target_conv' layer is designed as the Grad-CAM visualization target\n    \"\"\"\n    inputs = layers.Input(shape=input_shape)\n    base = tf.keras.applications.EfficientNetB0(include_top=False, input_tensor=inputs, weights='imagenet')\n    base.trainable = False  # freeze initially\n\n    # x shape: [B, 7, 7, 1280] from EfficientNetB0\n    x = base.output\n    \n    # === GRAD-CAM FRIENDLY ARCHITECTURE ===\n    # Apply 1x1 convolutions BEFORE pooling to maintain gradient path to spatial features\n    \n    # Primary Grad-CAM target layer - gradients flow here for visualization\n    x = layers.Conv2D(512, (1, 1), activation='relu', name='gradcam_target_conv')(x)  # [B, 7, 7, 512]\n    x = layers.BatchNormalization(name='gradcam_target_bn')(x)\n    \n    # Additional feature processing (still spatial)\n    x = layers.Conv2D(256, (1, 1), activation='relu', name='feature_conv')(x)  # [B, 7, 7, 256]\n    x = layers.BatchNormalization()(x)\n    \n    # GlobalAveragePooling - spatial info aggregated here\n    # Grad-CAM computes gradients w.r.t. conv layers BEFORE this\n    x = layers.GlobalAveragePooling2D(name='final_gap')(x)  # [B, 256]\n    \n    # Transformer on pooled features (single token)\n    token_dim = 256\n    x_tokens = layers.Reshape((1, x.shape[-1]))(x)  # [B, 1, 256]\n    \n    # Small transformer stack\n    for _ in range(2):\n        x_tokens = transformer_encoder(x_tokens, head_size=64, num_heads=4, ff_dim=512, dropout=0.1)\n    \n    # Classification head\n    x_flat = layers.LayerNormalization()(x_tokens)\n    x_flat = layers.Flatten()(x_flat)\n    x_flat = layers.Dropout(dropout_rate)(x_flat)\n    x_flat = layers.Dense(128, activation='relu')(x_flat)\n    x_flat = layers.Dropout(dropout_rate)(x_flat)\n    outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x_flat)\n\n    model = models.Model(inputs, outputs, name=\"EffNetB0_TransformerHead_GradCAM\")\n    return model\n\n# Build the new Grad-CAM friendly model\nmodel = build_hybrid_model_gradcam(input_shape=IMG_SIZE + (3,), num_classes=num_classes)\nmodel.summary()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸ”§ MODEL ARCHITECTURE MODIFIED FOR PROPER GRAD-CAM\")\nprint(\"=\"*70)\nprint(\"\"\"\nARCHITECTURE CHANGES:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  EfficientNetB0 (frozen)                                            â”‚\nâ”‚       â†“ [B, 7, 7, 1280]                                            â”‚\nâ”‚  Conv2D 1x1 (512) â†’ 'gradcam_target_conv'  â† GRAD-CAM TARGET       â”‚\nâ”‚       â†“ [B, 7, 7, 512]                                             â”‚\nâ”‚  Conv2D 1x1 (256) â†’ 'feature_conv'                                  â”‚\nâ”‚       â†“ [B, 7, 7, 256]                                             â”‚\nâ”‚  GlobalAveragePooling2D                                             â”‚\nâ”‚       â†“ [B, 256]                                                   â”‚\nâ”‚  Transformer (2 blocks)                                             â”‚\nâ”‚       â†“                                                            â”‚\nâ”‚  Dense â†’ Softmax                                                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nWHY THIS WORKS FOR GRAD-CAM:\nâ€¢ Gradients flow: Output â†’ Transformer â†’ GAP â†’ Conv layers â†’ Spatial features\nâ€¢ The conv layers BEFORE GAP preserve the spatial information needed for Grad-CAM\nâ€¢ 'gradcam_target_conv' has shape [7, 7, 512] - perfect for spatial visualization\n\nâš ï¸  IMPORTANT: This is a NEW model architecture - you need to RETRAIN!\n    Run cells 8 (compile) and 9 (train) to train this new model.\n\"\"\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T16:14:56.701606Z","iopub.execute_input":"2025-11-29T16:14:56.702010Z","iopub.status.idle":"2025-11-29T16:14:58.656300Z","shell.execute_reply.started":"2025-11-29T16:14:56.701978Z","shell.execute_reply":"2025-11-29T16:14:58.655466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Cell 8 â€” compile & callbacks\n# from tensorflow.keras import optimizers, callbacks\n\n# EPOCHS = 20\n# LR = 3e-4\n\n# model.compile(\n#     optimizer=optimizers.Adam(learning_rate=LR),\n#     loss='sparse_categorical_crossentropy',\n#     metrics=['accuracy']\n# )\n\n# os.makedirs(MODEL_DIR, exist_ok=True)\n# ckpt_path = os.path.join(MODEL_DIR, \"best_model.h5\")\n\n# cb_list = [\n#     callbacks.ModelCheckpoint(ckpt_path, monitor='val_accuracy', save_best_only=True, verbose=1),\n#     callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),\n#     callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n# ]\n\n# Cell 8 â€” compile & callbacks\nfrom tensorflow.keras import optimizers, callbacks\n\nEPOCHS = 20\nLR = 3e-4\n\nmodel.compile(\n    optimizer=optimizers.Adam(learning_rate=LR),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nos.makedirs(MODEL_DIR, exist_ok=True)\n\n# âœ” Save model in new Keras format to avoid legacy HDF5 warning\nckpt_path = os.path.join(MODEL_DIR, \"best_model.keras\")\n\ncb_list = [\n    callbacks.ModelCheckpoint(\n        filepath=ckpt_path,\n        monitor='val_accuracy',\n        save_best_only=True,\n        save_weights_only=False,   # Use full model (recommended)\n        mode='max',\n        verbose=1\n    ),\n    \n    callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=6,\n        restore_best_weights=True,\n        mode='min',\n        verbose=1\n    ),\n    \n    callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-6,\n        mode='min',\n        verbose=1\n    )\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T16:14:58.657362Z","iopub.execute_input":"2025-11-29T16:14:58.657631Z","iopub.status.idle":"2025-11-29T16:14:58.676263Z","shell.execute_reply.started":"2025-11-29T16:14:58.657601Z","shell.execute_reply":"2025-11-29T16:14:58.675306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9 â€” train\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    class_weight=class_weight,\n    callbacks=cb_list\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T16:14:58.678416Z","iopub.execute_input":"2025-11-29T16:14:58.679047Z","iopub.status.idle":"2025-11-29T17:55:52.241182Z","shell.execute_reply.started":"2025-11-29T16:14:58.679026Z","shell.execute_reply":"2025-11-29T17:55:52.239977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Cell 10 â€” Evaluate on validation set and compute classification report\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport itertools\n\n# collect predictions\ny_true = []\ny_pred = []\ny_prob = []\n\nfor batch_x, batch_y in val_ds:\n    preds = model.predict(batch_x)\n    y_true.extend(batch_y.numpy().tolist())\n    y_pred.extend(np.argmax(preds, axis=1).tolist())\n    y_prob.extend(np.max(preds, axis=1).tolist())\n\nprint(classification_report(y_true, y_pred, target_names=class_names))\ncm = confusion_matrix(y_true, y_pred)\nprint(\"Confusion matrix:\\n\", cm)\n\ndef plot_confusion_matrix(cm, classes):\n    plt.figure(figsize=(6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(\"Confusion matrix\")\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\nplot_confusion_matrix(cm, class_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T17:58:35.111781Z","iopub.execute_input":"2025-11-29T17:58:35.112184Z","iopub.status.idle":"2025-11-29T17:59:45.422303Z","shell.execute_reply.started":"2025-11-29T17:58:35.112157Z","shell.execute_reply":"2025-11-29T17:59:45.421474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11 â€” save predictions with filenames to CSV for RAG ingestion\nimport csv\nfrom pathlib import Path\n\n# Build mapping of filenames to labels for validation set\n# image_dataset_from_directory preserves order within batches but easier to re-walk val folder\nrows = []\nval_root = Path(VAL_DIR)\n# Walk classes and files\nfor class_idx, cls in enumerate(class_names):\n    p = val_root.joinpath(cls)\n    for img_path in p.glob(\"*\"):\n        rows.append({\"filepath\": str(img_path), \"true_label\": cls})\n\n# Note: val order here may not match the dataset order used above â€” so recompute preds by reading images file-by-file\ndef preprocess_image(path):\n    img = tf.io.read_file(str(path))\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.resize(img, IMG_SIZE)\n    img = tf.cast(img, tf.float32)\n    img = preprocess_input(img)\n    return img\n\nout_rows = []\nfor r in rows:\n    img = preprocess_image(r[\"filepath\"])\n    img_batch = tf.expand_dims(img, axis=0)\n    preds = model.predict(img_batch)\n    pred_idx = int(np.argmax(preds, axis=1)[0])\n    pred_label = class_names[pred_idx]\n    confidence = float(np.max(preds))\n    out_rows.append([r[\"filepath\"], r[\"true_label\"], pred_label, confidence])\n\ncsv_path = os.path.join(ARTIFACTS_DIR, \"val_predictions.csv\")\nwith open(csv_path, \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"filepath\", \"true_label\", \"pred_label\", \"confidence\"])\n    writer.writerows(out_rows)\n\nprint(\"Saved predictions CSV to:\", csv_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T19:16:06.982019Z","iopub.execute_input":"2025-11-29T19:16:06.982715Z","iopub.status.idle":"2025-11-29T19:19:14.741568Z","shell.execute_reply.started":"2025-11-29T19:16:06.982631Z","shell.execute_reply":"2025-11-29T19:19:14.740734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12 â€” Grad-CAM Setup for Modified Model\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import image as keras_image\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\n\nCLASS_NAMES = class_names\nprint(\"CLASS_NAMES:\", CLASS_NAMES)\n\n# Preprocessing function\ndef load_and_preprocess_for_model(img_path):\n    img = keras_image.load_img(img_path, target_size=IMG_SIZE)\n    x = keras_image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = effnet_preprocess(x)\n    return x\n\n# === USE THE NEW GRAD-CAM TARGET LAYER ===\n# The modified model has 'gradcam_target_conv' specifically designed for Grad-CAM\n# This layer is BEFORE GlobalAveragePooling, so gradients flow properly\n\nprint(\"\\nSearching for Grad-CAM target layer...\")\ntry:\n    # Use our new custom layer designed for Grad-CAM\n    gradcam_layer = model.get_layer('gradcam_target_conv')\n    print(f\"âœ“ Found gradcam_target_conv: {gradcam_layer.name}\")\nexcept:\n    # Fallback to EfficientNet's top_conv if using old model\n    gradcam_layer = model.get_layer('top_conv')\n    print(f\"Using fallback top_conv: {gradcam_layer.name}\")\n\nprint(f\"Output shape: {gradcam_layer.output.shape}\")\n\n# Build Grad-CAM model: outputs conv features AND predictions\ngrad_model = tf.keras.Model(\n    inputs=model.input,\n    outputs=[gradcam_layer.output, model.output]\n)\n\n# Verify the model works\nprint(\"\\nVerifying grad_model...\")\ndummy = np.zeros((1, 224, 224, 3), dtype=np.float32)\nconv_out, pred_out = grad_model(dummy, training=False)\nprint(f\"Conv output shape: {conv_out.shape}\")  # Should be [1, 7, 7, 512]\nprint(f\"Pred output shape: {pred_out.shape}\")  # Should be [1, 4]\nprint(\"âœ“ Grad-CAM model ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T19:19:14.743074Z","iopub.execute_input":"2025-11-29T19:19:14.743353Z","iopub.status.idle":"2025-11-29T19:19:15.302262Z","shell.execute_reply.started":"2025-11-29T19:19:14.743332Z","shell.execute_reply":"2025-11-29T19:19:15.301172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13 â€” Proper Gradient-Based Grad-CAM (works with modified architecture)\n# Now that we have conv layers BEFORE GAP, gradients flow properly!\n\ndef make_gradcam_heatmap(img_batch, target_class_idx, debug=False):\n    \"\"\"\n    Generate proper Grad-CAM heatmap using gradients.\n    \n    Works because our modified architecture has conv layers BEFORE GlobalAveragePooling,\n    allowing gradients to flow from the output back to spatial feature maps.\n    \"\"\"\n    img_batch = tf.cast(img_batch, tf.float32)\n    \n    with tf.GradientTape() as tape:\n        # Forward pass - get conv outputs and predictions\n        conv_outputs, predictions = grad_model(img_batch, training=False)\n        \n        # Watch conv_outputs for gradient computation\n        tape.watch(conv_outputs)\n        \n        # Get the score for the target class\n        class_score = predictions[:, target_class_idx]\n        \n        if debug:\n            print(f\"  Target class: {target_class_idx} ({CLASS_NAMES[target_class_idx]})\")\n            print(f\"  Class score: {class_score.numpy()[0]:.4f}\")\n    \n    # Compute gradients of class score w.r.t. conv outputs\n    grads = tape.gradient(class_score, conv_outputs)\n    \n    if grads is None:\n        print(\"ERROR: Gradients are None!\")\n        print(\"This means gradient flow is still broken.\")\n        print(\"Make sure you're using the modified model architecture.\")\n        # Return a uniform heatmap as fallback\n        return np.ones((7, 7), dtype=np.float32) * 0.5\n    \n    if debug:\n        print(f\"  âœ“ Gradients computed! Shape: {grads.shape}\")\n        print(f\"  Gradient range: [{grads.numpy().min():.6f}, {grads.numpy().max():.6f}]\")\n    \n    # Global Average Pool the gradients to get channel importance weights\n    # Shape: [1, 7, 7, channels] -> [1, channels]\n    pooled_grads = tf.reduce_mean(grads, axis=(1, 2))\n    \n    # Weight the conv outputs by the gradient importance\n    conv_outputs = conv_outputs[0]  # Remove batch dim: [7, 7, channels]\n    pooled_grads = pooled_grads[0]  # Remove batch dim: [channels]\n    \n    # Weighted combination: each channel weighted by its gradient importance\n    heatmap = tf.reduce_sum(conv_outputs * pooled_grads, axis=-1)  # [7, 7]\n    heatmap = heatmap.numpy()\n    \n    # Apply ReLU - we only care about features that positively affect the class\n    heatmap = np.maximum(heatmap, 0)\n    \n    # Normalize\n    if heatmap.max() > 0:\n        heatmap = heatmap / heatmap.max()\n    \n    if debug:\n        print(f\"  Heatmap range: [{heatmap.min():.3f}, {heatmap.max():.3f}]\")\n        if heatmap.max() > 0:\n            max_pos = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n            print(f\"  Max activation at: {max_pos}\")\n    \n    return heatmap\n\n\ndef resize_heatmap_to_image(heatmap, target_size):\n    \"\"\"Resize 7x7 heatmap to full image size with smoothing.\"\"\"\n    # Resize using bilinear interpolation\n    heatmap_resized = tf.image.resize(\n        heatmap[..., np.newaxis], \n        target_size, \n        method='bilinear'\n    )\n    heatmap_resized = tf.squeeze(heatmap_resized, axis=-1).numpy()\n    heatmap_resized = np.clip(heatmap_resized, 0, 1)\n    \n    # Apply slight smoothing for better visualization\n    heatmap_tf = tf.constant(heatmap_resized[None, ..., None], dtype=tf.float32)\n    heatmap_smooth = tf.nn.avg_pool2d(heatmap_tf, ksize=5, strides=1, padding='SAME')\n    heatmap_smooth = tf.squeeze(heatmap_smooth).numpy()\n    \n    if heatmap_smooth.max() > 0:\n        heatmap_smooth = heatmap_smooth / heatmap_smooth.max()\n    \n    return heatmap_smooth\n\n\n# === TEST GRADIENT FLOW ===\nprint(\"Testing gradient-based Grad-CAM...\")\nprint(\"=\"*50)\n\ntest_input = np.random.randn(1, 224, 224, 3).astype(np.float32)\ntest_heatmap = make_gradcam_heatmap(test_input, 0, debug=True)\n\nprint(\"=\"*50)\nif test_heatmap.max() > 0.5 or test_heatmap.min() != test_heatmap.max():\n    print(\"âœ“ SUCCESS! Gradients are flowing properly!\")\n    print(f\"  Heatmap shape: {test_heatmap.shape}\")\n    print(\"  Grad-CAM is working correctly with the modified architecture.\")\nelse:\n    print(\"âš  Heatmap may be uniform - check if gradients are valid.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T19:24:29.601708Z","iopub.execute_input":"2025-11-29T19:24:29.602523Z","iopub.status.idle":"2025-11-29T19:24:30.775737Z","shell.execute_reply.started":"2025-11-29T19:24:29.602491Z","shell.execute_reply":"2025-11-29T19:24:30.774919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main visualization function - Shows ONE heatmap for predicted class\ndef visualize_gradcam_single(img_path, true_label=None, alpha=0.6, save_path=None):\n    \"\"\"\n    Generate and display a single Grad-CAM heatmap for the predicted class.\n    Shows what regions the model focuses on to make its prediction.\n    \"\"\"\n    # Load and preprocess image\n    img_batch = load_and_preprocess_for_model(img_path)\n    \n    # Get model prediction\n    predictions = model.predict(img_batch, verbose=0)[0]\n    pred_idx = int(np.argmax(predictions))\n    pred_label = CLASS_NAMES[pred_idx]\n    pred_confidence = float(predictions[pred_idx])\n    \n    # Generate Grad-CAM heatmap for the PREDICTED class\n    heatmap_lowres = make_gradcam_heatmap(img_batch, pred_idx)\n    print(f\"  Low-res heatmap shape: {heatmap_lowres.shape}, range: [{heatmap_lowres.min():.2f}, {heatmap_lowres.max():.2f}]\")\n    \n    heatmap = resize_heatmap_to_image(heatmap_lowres, IMG_SIZE)\n    \n    # Load original image for display (NOT preprocessed)\n    original_img = keras_image.load_img(img_path, target_size=IMG_SIZE)\n    original_img = keras_image.img_to_array(original_img) / 255.0\n    \n    # Determine if it's a tumor or not\n    is_tumor = pred_label.lower() != 'notumor'\n    \n    # Create figure\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # 1. Original image\n    axes[0].imshow(original_img)\n    axes[0].set_title('Original MRI Scan', fontsize=12, fontweight='bold')\n    axes[0].axis('off')\n    \n    # 2. Heatmap visualization\n    if is_tumor:\n        # For tumor: show heatmap highlighting tumor region with 'hot' colormap\n        im = axes[1].imshow(heatmap, cmap='jet', vmin=0, vmax=1)\n        axes[1].set_title(f'Tumor Location ({pred_label})', fontsize=12, fontweight='bold', color='darkred')\n        \n        # Overlay - show tumor region on MRI\n        axes[2].imshow(original_img)\n        # Use threshold to show only high-activation areas\n        heatmap_thresh = np.where(heatmap > 0.3, heatmap, 0)\n        axes[2].imshow(heatmap_thresh, cmap='jet', alpha=alpha, vmin=0, vmax=1)\n        axes[2].set_title('Tumor Region Highlighted', fontsize=12, fontweight='bold', color='darkred')\n    else:\n        # For no tumor: show what model examined\n        im = axes[1].imshow(heatmap, cmap='Greens', vmin=0, vmax=1)\n        axes[1].set_title('Model Focus Area', fontsize=12, fontweight='bold', color='darkgreen')\n        \n        axes[2].imshow(original_img)\n        axes[2].imshow(heatmap, cmap='Greens', alpha=alpha * 0.6, vmin=0, vmax=1)\n        axes[2].set_title('No Tumor - Healthy Scan', fontsize=12, fontweight='bold', color='darkgreen')\n    \n    axes[1].axis('off')\n    axes[2].axis('off')\n    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n    \n    # Build main title\n    if true_label is not None:\n        correct = \"âœ“ Correct\" if true_label == pred_label else \"âœ— Incorrect\"\n        main_title = f\"{correct} | True: {true_label} | Predicted: {pred_label} ({pred_confidence:.1%})\"\n        title_color = 'green' if true_label == pred_label else 'red'\n    else:\n        main_title = f\"Predicted: {pred_label} ({pred_confidence:.1%})\"\n        title_color = 'darkred' if is_tumor else 'darkgreen'\n    \n    fig.suptitle(main_title, fontsize=14, fontweight='bold', color=title_color)\n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        print(f\"Saved to: {save_path}\")\n    \n    plt.show()\n    \n    # Print class probabilities\n    print(f\"\\nClass Probabilities:\")\n    for idx, class_name in enumerate(CLASS_NAMES):\n        prob = predictions[idx]\n        bar = 'â–ˆ' * int(prob * 30)\n        marker = \" â† Predicted\" if idx == pred_idx else \"\"\n        print(f\"  {class_name:12s}: {prob:.4f} {bar}{marker}\")\n    \n    # Print interpretation\n    if is_tumor:\n        print(f\"\\nâš ï¸  TUMOR DETECTED: {pred_label.upper()}\")\n        print(f\"   The highlighted regions show where the model identified tumor characteristics.\")\n    else:\n        print(f\"\\nâœ“  NO TUMOR DETECTED\")\n        print(f\"   The model examined the brain tissue and found no signs of tumor.\")\n    \n    return pred_label, pred_confidence, heatmap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T19:24:33.574408Z","iopub.execute_input":"2025-11-29T19:24:33.575165Z","iopub.status.idle":"2025-11-29T19:24:33.592643Z","shell.execute_reply.started":"2025-11-29T19:24:33.575134Z","shell.execute_reply":"2025-11-29T19:24:33.591430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Batch visualization from validation set\ndef visualize_batch_gradcam(num_samples=6, include_all_classes=True):\n    \"\"\"\n    Visualize Grad-CAM for multiple samples from each class.\n    \n    Args:\n        num_samples: Total number of samples to visualize\n        include_all_classes: If True, sample from each class for diversity\n    \"\"\"\n    import os\n    import random\n    \n    # Collect images from training/validation directory\n    data_dir = VAL_DIR if os.path.exists(VAL_DIR) else TRAIN_DIR\n    \n    if include_all_classes:\n        # Sample from each class for diversity\n        samples_per_class = max(1, num_samples // len(CLASS_NAMES))\n        sampled = []\n        \n        for cls_name in CLASS_NAMES:\n            cls_path = os.path.join(data_dir, cls_name)\n            if not os.path.isdir(cls_path):\n                continue\n            \n            images = [f for f in os.listdir(cls_path) \n                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            \n            if images:\n                selected = random.sample(images, min(samples_per_class, len(images)))\n                for img_name in selected:\n                    sampled.append((os.path.join(cls_path, img_name), cls_name))\n        \n        # Shuffle and limit to requested number\n        random.shuffle(sampled)\n        sampled = sampled[:num_samples]\n    else:\n        # Random sampling across all classes\n        sampled = []\n        for cls_name in CLASS_NAMES:\n            cls_path = os.path.join(data_dir, cls_name)\n            if os.path.isdir(cls_path):\n                for img_name in os.listdir(cls_path):\n                    if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n                        sampled.append((os.path.join(cls_path, img_name), cls_name))\n        \n        sampled = random.sample(sampled, min(num_samples, len(sampled)))\n    \n    print(f\"Generating Grad-CAM visualizations for {len(sampled)} images...\")\n    print(\"=\" * 70)\n    \n    results = []\n    for idx, (img_path, true_label) in enumerate(sampled, 1):\n        print(f\"\\n[{idx}/{len(sampled)}] {os.path.basename(img_path)}\")\n        pred_label, confidence, heatmap = visualize_gradcam_single(img_path, true_label)\n        results.append({\n            'path': img_path,\n            'true_label': true_label,\n            'pred_label': pred_label,\n            'confidence': confidence,\n            'correct': true_label == pred_label\n        })\n        print(\"-\" * 70)\n    \n    # Summary\n    correct = sum(1 for r in results if r['correct'])\n    print(f\"\\n{'='*70}\")\n    print(f\"Summary: {correct}/{len(results)} correct predictions ({100*correct/len(results):.1f}% accuracy)\")\n    \n    return results\n\n\n# Run visualization\nprint(\"Generating Grad-CAM visualizations for brain tumor classification...\")\nprint(\"Each visualization shows ONE heatmap for the predicted class.\\n\")\n\nresults = visualize_batch_gradcam(num_samples=6, include_all_classes=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T19:24:36.747764Z","iopub.execute_input":"2025-11-29T19:24:36.748122Z","iopub.status.idle":"2025-11-29T19:24:44.023892Z","shell.execute_reply.started":"2025-11-29T19:24:36.748093Z","shell.execute_reply":"2025-11-29T19:24:44.022763Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test on a single specific image\n# Replace the path with any image you want to analyze\n\n# Example: using a path from the validation set\nimport os\nimport random\n\n# Find an example image\nexample_path = None\nif os.path.exists(VAL_DIR):\n    for cls_name in class_names:\n        cls_path = os.path.join(VAL_DIR, cls_name)\n        if os.path.isdir(cls_path):\n            images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            if images:\n                example_path = os.path.join(cls_path, images[0])\n                example_label = cls_name\n                break\n\nif example_path:\n    print(\"Single Image Grad-CAM Analysis\")\n    print(\"=\" * 50)\n    pred_label, confidence, heatmap = visualize_gradcam_single(\n        example_path, \n        true_label=example_label,\n        alpha=0.5\n    )\nelse:\n    print(\"No validation images found. Skipping single image test.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T19:25:16.950326Z","iopub.execute_input":"2025-11-29T19:25:16.951008Z","iopub.status.idle":"2025-11-29T19:25:18.729254Z","shell.execute_reply.started":"2025-11-29T19:25:16.950976Z","shell.execute_reply":"2025-11-29T19:25:18.728368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize from CSV predictions file (if available)\nimport pandas as pd\nimport os\n\nif os.path.exists(csv_path):\n    df = pd.read_csv(csv_path)\n    print(f\"Loaded {len(df)} predictions from CSV\")\n    \n    # Sample diverse images from CSV\n    sample_df = df.groupby('true_label', group_keys=False).apply(\n        lambda x: x.sample(min(2, len(x)), random_state=42)\n    ).reset_index(drop=True)\n    \n    sample_df = sample_df.head(6)  # Limit to 6 total\n    \n    print(f\"\\nVisualizing {len(sample_df)} samples from validation predictions:\")\n    print(\"=\" * 70)\n    \n    for idx, row in sample_df.iterrows():\n        print(f\"\\n[{idx+1}/{len(sample_df)}] {os.path.basename(row['filepath'])}\")\n        visualize_gradcam_single(\n            row['filepath'], \n            true_label=row['true_label'],\n            alpha=0.5\n        )\n        print(\"-\" * 70)\nelse:\n    print(f\"CSV file not found at: {csv_path}\")\n    print(\"Run the prediction cell first to generate the CSV.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T19:25:31.575287Z","iopub.execute_input":"2025-11-29T19:25:31.575633Z","iopub.status.idle":"2025-11-29T19:25:42.609419Z","shell.execute_reply.started":"2025-11-29T19:25:31.575608Z","shell.execute_reply":"2025-11-29T19:25:42.608426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Cell 13 â€” Fine-tune some top EfficientNet layers\n# # The EfficientNet layers are directly in model.layers (not nested) when using input_tensor\n\n# # Method 1: Find EfficientNet layers by name pattern\n# efficientnet_layers = [l for l in model.layers if 'efficientnet' in l.name.lower() or \n#                        l.name.startswith('block') or \n#                        l.name.startswith('stem') or\n#                        l.name == 'top_conv' or\n#                        l.name == 'top_bn']\n\n# # Method 2: More robust - find by layer type patterns common in EfficientNet\n# if not efficientnet_layers:\n#     efficientnet_layers = [l for l in model.layers \n#                           if any(x in l.name for x in ['block', 'stem', 'top_conv', 'top_bn', 'mbconv'])]\n\n# print(f\"Found {len(efficientnet_layers)} EfficientNet-related layers\")\n\n# if len(efficientnet_layers) == 0:\n#     print(\"No EfficientNet layers found. Listing all model layers:\")\n#     for i, l in enumerate(model.layers):\n#         print(f\"  {i}: {l.name} ({l.__class__.__name__})\")\n#     print(\"\\nSkipping fine-tune.\")\n# else:\n#     # Print which layers we found\n#     print(\"EfficientNet layers found (last 10):\")\n#     for l in efficientnet_layers[-10:]:\n#         print(f\"  - {l.name} (trainable: {l.trainable})\")\n    \n#     # Unfreeze the last N EfficientNet layers (avoid BatchNorm)\n#     UNFREEZE_COUNT = 40  # unfreeze last 40 EfficientNet layers\n#     layers_to_unfreeze = efficientnet_layers[-UNFREEZE_COUNT:]\n    \n#     unfrozen_count = 0\n#     for layer in layers_to_unfreeze:\n#         # Skip BatchNormalization layers (keep them frozen for stability)\n#         if not isinstance(layer, tf.keras.layers.BatchNormalization):\n#             layer.trainable = True\n#             unfrozen_count += 1\n    \n#     print(f\"\\nâœ“ Unfroze {unfrozen_count} layers for fine-tuning\")\n    \n#     # Also ensure our custom layers are trainable\n#     for layer in model.layers:\n#         if layer.name in ['gradcam_target_conv', 'gradcam_target_bn', 'feature_conv']:\n#             layer.trainable = True\n#             print(f\"  + {layer.name} set to trainable\")\n    \n#     # Compile with a lower learning rate\n#     model.compile(\n#         optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n#         loss='sparse_categorical_crossentropy',\n#         metrics=['accuracy']\n#     )\n    \n#     print(\"\\nðŸ“‹ Model compiled for fine-tuning with lr=1e-5\")\n    \n#     # Fine-tune training\n#     ft_ckpt = os.path.join(MODEL_DIR, \"best_model_finetuned.h5\")\n#     ft_history = model.fit(\n#         train_ds,\n#         validation_data=val_ds,\n#         epochs=10,\n#         class_weight=class_weight,\n#         callbacks=[\n#             callbacks.ModelCheckpoint(ft_ckpt, monitor='val_accuracy', save_best_only=True),\n#             callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n#         ]\n#     )\n#     print(\"âœ“ Fine-tune complete. Best weights saved to:\", ft_ckpt)\n\n# Cell 13 â€” Fine-tune some top EfficientNet layers\n# The EfficientNet layers are directly in model.layers (not nested) when using input_tensor\n\nfrom tensorflow.keras import callbacks  # ensure callbacks imported\n\n# Method 1: Find EfficientNet layers by name pattern\nefficientnet_layers = [\n    l for l in model.layers\n    if 'efficientnet' in l.name.lower()\n    or l.name.startswith('block')\n    or l.name.startswith('stem')\n    or l.name == 'top_conv'\n    or l.name == 'top_bn'\n]\n\n# Method 2: More robust - find by layer type/name patterns common in EfficientNet\nif not efficientnet_layers:\n    efficientnet_layers = [\n        l for l in model.layers\n        if any(x in l.name for x in ['block', 'stem', 'top_conv', 'top_bn', 'mbconv'])\n    ]\n\nprint(f\"Found {len(efficientnet_layers)} EfficientNet-related layers\")\n\nif len(efficientnet_layers) == 0:\n    print(\"No EfficientNet layers found. Listing all model layers:\")\n    for i, l in enumerate(model.layers):\n        print(f\"  {i}: {l.name} ({l.__class__.__name__})\")\n    print(\"\\nSkipping fine-tune.\")\nelse:\n    # Print which layers we found\n    print(\"EfficientNet layers found (last 10):\")\n    for l in efficientnet_layers[-10:]:\n        print(f\"  - {l.name} (trainable: {l.trainable})\")\n    \n    # Unfreeze the last N EfficientNet layers (avoid BatchNorm)\n    UNFREEZE_COUNT = 40  # unfreeze last 40 EfficientNet layers\n    layers_to_unfreeze = efficientnet_layers[-UNFREEZE_COUNT:]\n    \n    unfrozen_count = 0\n    for layer in layers_to_unfreeze:\n        # Skip BatchNormalization layers (keep them frozen for stability)\n        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n            layer.trainable = True\n            unfrozen_count += 1\n    \n    print(f\"\\nâœ“ Unfroze {unfrozen_count} layers for fine-tuning\")\n    \n    # Also ensure our custom layers are trainable\n    for layer in model.layers:\n        if layer.name in ['gradcam_target_conv', 'gradcam_target_bn', 'feature_conv']:\n            layer.trainable = True\n            print(f\"  + {layer.name} set to trainable\")\n    \n    # Compile with a lower learning rate for fine-tuning\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    print(\"\\nðŸ“‹ Model compiled for fine-tuning with lr=1e-5\")\n    \n    # Fine-tune training â€” use new Keras format for checkpoint\n    ft_ckpt = os.path.join(MODEL_DIR, \"best_model_finetuned.keras\")\n\n    ft_callbacks = [\n        callbacks.ModelCheckpoint(\n            filepath=ft_ckpt,\n            monitor='val_accuracy',\n            save_best_only=True,\n            save_weights_only=False,\n            mode='max',\n            verbose=1\n        ),\n        callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True,\n            mode='min',\n            verbose=1\n        ),\n        callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=2,\n            min_lr=1e-7,\n            mode='min',\n            verbose=1\n        )\n    ]\n\n    ft_history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=10,\n        class_weight=class_weight,\n        callbacks=ft_callbacks\n    )\n    print(\"âœ“ Fine-tune complete. Best weights saved to:\", ft_ckpt)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T19:26:04.196065Z","iopub.execute_input":"2025-11-29T19:26:04.196401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Quick test prediction\n# Find an example image from training set\n\nimport os\n\ntest_img_path = None\nif os.path.exists(TRAIN_DIR):\n    for cls_name in class_names:\n        cls_path = os.path.join(TRAIN_DIR, cls_name)\n        if os.path.isdir(cls_path):\n            images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            if images:\n                test_img_path = os.path.join(cls_path, images[0])\n                break\n\nif test_img_path:\n    img_batch = load_and_preprocess_for_model(test_img_path)\n    preds = model.predict(img_batch)\n    print(f\"Test image: {os.path.basename(test_img_path)}\")\n    print(f\"Predictions: {preds}\")\n    print(f\"Predicted class: {class_names[np.argmax(preds)]} (index: {np.argmax(preds)})\")\nelse:\n    print(\"No test image found.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Cell 14 â€” Save final model and plots\n# final_model_path = os.path.join(MODEL_DIR, \"final_model.h5\")\n# model.save(final_model_path)\n# print(\"Saved final model to:\", final_model_path)\n\n# # Plot history if available (initial training)\n# def plot_history(history, fname_prefix=\"history\"):\n#     import matplotlib.pyplot as plt\n#     hist = history.history\n#     if 'accuracy' in hist:\n#         plt.figure(figsize=(8,4))\n#         plt.plot(hist['accuracy'], label='train_acc')\n#         plt.plot(hist['val_accuracy'], label='val_acc')\n#         plt.title('Accuracy')\n#         plt.legend()\n#         plt.savefig(os.path.join(MODEL_DIR, f\"{fname_prefix}_acc.png\"))\n#         plt.close()\n#     if 'loss' in hist:\n#         plt.figure(figsize=(8,4))\n#         plt.plot(hist['loss'], label='train_loss')\n#         plt.plot(hist['val_loss'], label='val_loss')\n#         plt.title('Loss')\n#         plt.legend()\n#         plt.savefig(os.path.join(MODEL_DIR, f\"{fname_prefix}_loss.png\"))\n#         plt.close()\n\n# # call for initial training history\n# try:\n#     plot_history(history, \"initial\")\n# except NameError:\n#     pass\n\n# try:\n#     plot_history(ft_history, \"finetune\")\n# except NameError:\n#     pass\n\n# print(\"Saved training plots (if histories exist) into:\", MODEL_DIR)\n\n\n\n# Cell 14 â€” Save final model and plots\nfinal_model_path = os.path.join(MODEL_DIR, \"final_model.keras\")  # âœ” modern format\nmodel.save(final_model_path)\nprint(\"Saved final model to:\", final_model_path)\n\n# Plot history if available\ndef plot_history(history, fname_prefix=\"history\"):\n    import matplotlib.pyplot as plt\n    hist = history.history\n\n    if 'accuracy' in hist:\n        plt.figure(figsize=(8,4))\n        plt.plot(hist['accuracy'], label='train_acc')\n        plt.plot(hist.get('val_accuracy', []), label='val_acc')\n        plt.title('Accuracy')\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(MODEL_DIR, f\"{fname_prefix}_acc.png\"))\n        plt.close()\n\n    if 'loss' in hist:\n        plt.figure(figsize=(8,4))\n        plt.plot(hist['loss'], label='train_loss')\n        plt.plot(hist.get('val_loss', []), label='val_loss')\n        plt.title('Loss')\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(MODEL_DIR, f\"{fname_prefix}_loss.png\"))\n        plt.close()\n\n# Safely plot training histories (if they exist)\ntry:\n    plot_history(history, \"initial\")\nexcept NameError:\n    pass\n\ntry:\n    plot_history(ft_history, \"finetune\")\nexcept NameError:\n    pass\n\nprint(\"Saved training plots (if histories exist) into:\", MODEL_DIR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 15 â€” Notes for RAG Integration\n# Use the predictions CSV produced at:\n#   {ARTIFACTS_DIR}/val_predictions.csv\n# For each row you have: filepath, true_label, pred_label, confidence\n# \n# RAG ingestion steps (high-level):\n# 1. Build an index (FAISS) of literature + prior reports (text chunks).\n# 2. For each prediction row, retrieve top-k supporting text snippets from the index.\n# 3. Create a prompt containing: anonymized patient id, pred_label & confidence, top-k retrievals, any Grad-CAM notes.\n# 4. Send the prompt to the LLM to generate a structured clinical report.\n# 5. Use n8n to automate the pipeline: webhook -> inference -> retrieval -> LLM -> save/send report.\n\nprint(\"âœ“ Notebook complete!\")\nprint(f\"Models saved to: {MODEL_DIR}\")\nprint(f\"Reports/artifacts saved to: {REPORT_DIR} and {ARTIFACTS_DIR}\")\nprint(f\"Predictions CSV: {os.path.join(ARTIFACTS_DIR, 'val_predictions.csv')}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}